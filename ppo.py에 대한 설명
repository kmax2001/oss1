ppo 코드 분석

O PPO 클래스의 구조
1.기본 구조 (생성함수)
-
	PPO(actor,
	critic,
	num_envs,
	num_transitions_per_env,
	num_learning_epochs,
	num_mini_batches,
	clip_param=0.2,
	gamma=0.998,
	lam=0.95,
	value_loss_coef=0.5,
	entropy_coef=0.0,
	learning_rate=5e-4,
	max_grad_norm=0.5,
	use_clipped_value_loss=True,
	log_dir='run',
	device='cpu',
	mini_batch_sampling='shuffle',
	log_intervals=10

-멤버: actor, critic, storage 등등
2. 생성함수의 기능
-actor에 actor, critic에 critic 대입.
-storage에 RolloutStorage(num_envs, num_transitions_per_env, actor.obs_shape, critic.obs_shape, actor.action_shape, device) 대입
- mini_batch_sampling인자에 'shuffle', 또는 'in_order'가 들어갈 수 있으며, 이에 맞게 storage에 mini_batch를 생성함.
- optimizer에 optim.Adam([*self.actor.parameters(), *self.critic.parameters()], lr=learning_rate)
- device, n_t_p_e, n_e, clip_param, num_learning_epochs, num_mini_batches, value_loss_coef, entropy_coef, gamma, lam, max_grad_norm, use_clipped_value_loss에 각 인자로 받은 값을 대입

- log_dir에 'run/Jan12_15-42-44'과 같은 형식으로 저장
- writer에 summarywriter(log_dir=self.log_dir, flush_secs=10)으로써 저장
- tot_timesteps, tot_time을 0으로 초기화. ep_infos에는 빈 리스트를 저장. log_intervals에는 인자로 받은 log_intervals(default값 10)을 저장

- actions, actions_log_prob, actor_obs에는 None으로 초기화


3. 멤버함수 observe(self, actor_obs)
- 멤버변수 actor_obs를 actor_obs로 초기화
- actions와 actions_log_prob에 self.actor.sample(torch.from_numpy(actor_obs).to(self.device))을 대입
- actions().cpu().numpy()를 반환

4. 멤버함수 step(self, value_obs, rews, dones, infos)
- values에 self.critic.predict(torch.from_numpy(value_obs).to(self.device))대입
- storage에 add_transitions 실행 -> 하나의 trajectory를 만드는 듯
- infos는 아마 info 딕셔너리의 리스트인 듯. infos의 각 info딕셔너리의 'episode'키의 값을 ep_info에 대입, 
self.ep_infos리스트에 ep_info 대입


5. 멤버함수 update(self, actor_obs, value_obs, log_this_iteration, update)
- last_values에 self.critic.predict(torch.from_numpy(value_obs).to(self.device))대입
- self.storage의 rolloutstorage 객체에 대하여 return값 계산함.
- rolloutstorage.clear() -> step을 0으로 초기화
- ep_infos.clear() -> ep_infos를 초기화함

6. 멤버함수 log(self, variables, width=80, pad=28)
- tot_timesteps에 += self.num_transitions_per_env * self.num_envs 연산 실행
- 생략: 추후에 필요하에 다시 볼 것임

7. 멤버함수 _train_step(self)
- ppo에 대한 과정이 들어있음. 추후에 ppo알고리즘을 한번 더 이해할 때, 코드 각 줄을 ppo와 함께 이해하면 좋을 듯
- mean_value_loss값과, mean_surrogate_loss값, locals()반환


=========================================궁금한 점===============================================
- n_t_p_e와 n_e가 무엇을 의미하는 것인가?
- 도대체 actor.sample등은 어디서 찾을 수 있는 것인가
- mini_batch_generator_shuffle에서 순서를 뒤바꾸면 state transition을 무시하게 되는 것 아닌가? 
====================================================================================================
O RolloutStorage에 대한 설명(생성함수 위주로)
1. 생성함수
- .storage.py의 클래스
- RolloutStorage(num_envs, num_transitions_per_env, actor.obs_shape, critic.obs_shape, actor.action_shape, device)의 형식
- 멤버: critic_obs, actor_obs, rewards, actions, dones, actions_log_prob, values, returns, advantages, num_transitions_per_env, num_envs, device, step 등이 있음

- critic_obs와 actor_obs각각 num_transitions*num_envs*(각 멤버 object의 형태)크기의 영행렬, 두 멤버에 데이터를 제공하는 입력에는 to(self.device)를 뒤에 붙여야함.
- rewards, actions, dones, 또한 n_t*n_envs에 각각 1, action_shape, 1을 곱한 크기의 영행렬 대입. 
- actions_log_prob, values returns, advantanges도 위의 곱에 1을 곱한 크기의 영행렬
- n_t_p_e,n_e에는 해당하는 생성함수의 인자를 대입 
- step에는 0대입

2. 멤버함수 add_transitions(self, actor_obs, critic_obs, actions, rewards, dones, values, actions_log_prob)

- step이 num_transitions_per_env보다 작아야 함
- 멤버 critic_obs, actor_obs, actions, rewards, dones, values, actions_log_prob[self.step]에 멤버함수의 인자들을 대입함.
- 특히 rewards, dones, actions_log_prob에는 인자를 열벡터로 바꾸어 대입 
- step에 1을 더함

3. 멤버함수 clear(self)
- step을 0으로 바꿈

4. 멤버함수 compute_returns(self, last_values, gamma, lam)
- advantage 변수를 0으로 초기화
- delta = self.rewards[step] + next_is_not_terminal * gamma * next_values - self.values[step]
- advantage = delta + next_is_not_terminal * gamma * lam * advantage
- returns[step]= advantage + self.values[step]을 이용하여 계산
-> 각 step의 state-action에 해당하는 return을 계산

5. 멤버함수  mini_batch_generator_shuffle(self, num_mini_batches)
- batch_size= n_e와 n_t_p_e의 곱(아마 n_e는 trajectory의 수 이고, n_t_p_e는 한 trajectory당 s-a의 수인듯?)
- mini_batch_size는 batch_size를 num_mini_batches로 나눈 것. 
- for indices Batch~~: range(batch_size)의 수들을 무작위로 한 번만 사용하여 mini_batch_size크기의 list를 만듦. indices는 각각의 list로 하여 반복문 실행함. 
- 즉 n_e*n_t_p_e*shape형태로 되어있는 actor와 critic을 batch_size*shape로 늘어놓고 indices list의 원소에 해당하는 index에 해당하는 actor_obs, critic_obs, actions, values, returns, action_log_prob, advantages 값들을 취하여 mini_batch_size크기의 리스트를 각각 만듦. 
- 각 리스트들을 튜블로 묶어 yield함. 이 튜플이 num_batch_size만큼 생김. 

6. 멤버함수 def mini_batch_generator_inorder(self, num_mini_batches)
- batch를 차례대로 묶은 것, 위와 비슷한 메커니즘.

====================================================================================
O module.py에 대한 설명
1. class Actor
- 멤버함수 sample: sample한 action과 그 log_prob값을 반환


